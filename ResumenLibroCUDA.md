# Introducción a los Kernels y Hardware del GPU 

Para iniciar este curso, el autor nos muestra un ejemplo para realizar una sencilla suma pero con 3 métodos diferentes, primero con la CPU, después con la CPU de nuevo pero ahora usando los threads del procesador y al final con la GPU usando CUDA.

Nos comenta que las computadoras modernas cuentan con procesadores con dos, 4 o más núcleos, y que para sacar el mayor rendimiento de nuestro hardware, nuestro código tiene que exprimir cada posibilidad de usar la programación paralela. Actualmente existen herramientas como OpenMP o la clase <thread> en C++11 que nos permiten programar en paralelo aprovechando el número de núcleos de nuestro procesador. Sin embargo, para poder alcanzar un rendimiento equivalente a 200 veces su rendimiento normal, tendríamos que tener aproximadamente 25 computadoras con CPUs de 8 núcleos, lo que definitivamente sería muy caro y el gasto de energía sería inmenso.
Para evitar esto, podemos instalarle una GPU a nuestra computadora y utilizar sus capacidades para realizar exactamente lo mismo. Con solo una computadora podemos aumentar el rendimiento de un proceso 200 veces o incluso más con un poco de esfuerzo pero definitivamente más barato y menos consumo energético. Además, una ventaja extra de una GPU es que su memoria interna es 10 veces más rápida que la de una computadora convencional.

Los programas que usan CUDA siempre llevarán una función Kernel, ya que esta es la que se comunica con la GPU. Las funciones Kernel están escritas en C++ con unas cuantas extensiones y restricciones. Antes de empezar a explicar los ejemplos de los programas anteriormente mencionados, el autor recalca que es muy importante conocer las capacidades de nuestro hardware y entender cómo opera para aprovechar el mayor rendimiento, pero eso vendrá después.

Para los ejemplos, el autor realiza una suma de integrales mediante las series de Taylor, evaluando el seno de x desde 0 hasta pi. Realizar un cálculo de seno es pesado para la computación. El usuario puede declarar la cantidad de iteraciones que se pueden realizar para obtener un resultado con mayor precisión.
## Ejemplo en CPU sin programación paralela

En el primer ejemplo, se realiza el cálculo mediante la CPU usando las librerías <stdio.h> para manejar entradas y salidas de datos, <stdlib.h> para usar la función atoi() para convertir los valores string en valores int, y una librería personalizada para este libro llama “cxtimers.h” pero en sí esta nos trae las funciones que queremos usar de la librería <chrono.h> que nos sirve para medir el tiempo de ejecución del programa. Después declara una función de tipo float llamada sinsum(float x, int terms) que sirve para realizar el cálculo de seno y obtener su resultado. En la función main(), declaramos las variables de tipo int llamadas “steps” que será igual al valor que le demos en su argumento en caso de haber más de uno o por default será 1,000,000, y “terms” que será igual al valor que le demos en su argumento en caso de haber más de 2 o por default será 1000.
Se declaran las variables de tipo double de nombre “pi” que contiene el valor de pi y “step_size” que es igual a pi / (steps - 1). Después declara una clase timer usando el namespace cx de nombre “tim”. Declara otra variable de tipo double de nombre “cpu_sum” para guardar el resultado final.
Procede a usa una función for() para realizar la suma de las series de Taylos y, al finalizar este, crea otra variable de tipo double de nombre “cpu_time” para almacenar el tiempo que duró el programa en ejecución.
Al final, realiza unos cálculos para corregir el margen de error del programa e imprime el resultado que, efectivamente, se aproxima a 2 y duró un total de 1818.959 ms.
## Ejemplo en CPU usando programación paralela

En el segundo ejemplo, utilizó la librería OpenMP para realizar el cálculo compartiendo datos entre distintos threads del CPU simultáneamente. Afortunadamente, solo se tuvieron que agregar 4 líneas más al código anterior. La primera para agregar la librería <omp.h> (OpenMP), la segunda para declarar la variable threads, que sirve para definir la cantidad de threads que queremos usar según nuestra CPU, la tercera que sirve para decirle a OpenMP cuantos threads queremos que use, que en este caso usamos la variable anteriormente mencionada. Y por último pero muy importante, se declara un pragma que sirve para declarar lo que queremos que se ejecute de manera paralela que en este caso es la siguiente función for() con que se tope. Lo que hace es dividirlo en sub-loops para ejecutarlos en los respectivos threads asignados. Un detalle muy importante es que el resultado final no depende del orden de las sumas, sino que cada sub-loop va almacenando la suma parcial del resultado final para que al final se termine sumando todo. Además, la última parte del pragma le indica a OpenMP que se trata de una operación reduction (usando adición) en la variable omp_sum. OpenMP sumará los resultados parciales de cada thread y colocará el resultado en la variable omp_sum cuando termine el loop.
Cabe aclarar que en este código, la variable cpu_sum fue sustituida por omp_sum, con fines de saber que clase de suma se realizó.
Este proceso duró un total de 508.635 ms.

## Ejemplo en GPU usando CUDA

Lo bonito de este caso, es que no hay que realizar muchos cambios, ya que CUDA está hecho en C++ con unas keywords extra.

En este tercer ejemplo.
* Utilizamos la librería “cuda_runtime.h” que siempre tiene que llevar un programa de CUDA.
* Agregamos la librería “thrust/device_vector.h” que sirve para utilizar vectores, de hecho, es muy parecido a los objetos std::vector, pero CUDA tiene distintas clases, una para la CPU y la otra para la GPU.
* La función sinsum() es la misma que en ejemplos pasados, solo que la diferencia es que ahora vienen con la declaración __host__ y __device__. Lo que hace es decirle al compilador que haga 2 versiones de la función, una que se va a ejecutar en la CPU y la otra se ejecutará en GPU.
* Ahora definimos una función CUDA Kernel llama gpu_sin que básicamente reemplaza al loop que usábamos. En lugar de usar una pequeña cantidad de threads como en el programa original, CUDA usa enorme cantidad de threads del GPU. Las funciones Kernel son declaradas con el keyword __global__ y se ejecutan por el host code (el procesador). Estas pueden recibir argumentos desde el host pero no pueden retornar valores. Se pueden pasar argumentos como valores o apuntadores que previamente se han declarado en memoria. No se pueden pasar argumentos mediante referencias, al igual que la GPU no puede acceder a los valores almacenados en la memoria del host.
* Los resultados de la función Kernel se guardan de manera paralela un un arreglo que se encuentra en la memoria global de la GPU, en lugar de sumarlo de manera secuencial.
* Definimos 2 nuevas variables, “threads y blocks”. Estás variables también estarán en cualquier programa de CUDA. Las GPU de Nvidia procesan los threads en blocks. threads define el número de threads en cada block y blocks el número de blocks a usar. El valor de threads tiene que ser un múltiplo de 32, y el número de block puede ser muy grande.
* Colocamos un array de nombre dsum en la GPU. Como dije, funciona como std::vector excepto que es la clase thrust de CUDA. Este array se inicializa en cero en el device (GPU).
* Creamos un apuntador de nombre “dptr” que lleve a la memoria de “dsum”. Nos sirve ya que es un argumento válido para las funciones Kernel.
* Ejecutamos el Kernel gpu_sin, el cual usa steps para separar las threads del GPU, en la cual, cada una llama la función sinsum() para todos los valores x requeridos en paralelo. Almacena los resultados en el array dsums. Por último, llama a la función reduce del thrust vector para sumar todos los resultados y guardarlos en la variable dsums, para terminar realizando una copia del resultado desde la GPU devuelta a la variable dsum del host.
## Arquitectura del CPU

* Master Clock: Básicamente es el que ejecuta las instrucciones con velocidad directamente proporcional a la frecuencia del procesador y lo hace mediante pulsos.
* Memory: La memoria principal guarda tanto los datos del programa como las instrucciones en lenguaje máquina que dropeó el compilador. Los datos en memoria pueden ser leídos por la misma memoria mediante la unidad load/save o por la unidad fetch del programa pero normalmente solo la unidad load/save puede escribir data devuelta en la memoria.
* Load/Save: Esta unidad lee y envía datos a la memoria principal. Esta unidad es controlada por la lógica del Execute dónde en cada paso especifica si el dato será leído o escrito para la memoria principal. Los datos en cuestión son movidos desde o para uno de los registros en el register file.
* Register File: Los datos se tienen que guardar en uno o más registros para poder ser operados por el ALU
* ALU o Arithmetic Logic Unit: Este dispositivo realiza operaciones aritméticas y lógicas en los datos guardados en registros. En cada paso la unidad execute es la que especifica la operación requerida.
* Execute: Esta unidad codifica las instrucciones enviadas por la unidad fetch, organiza la transferencia de input data hacia el register file, le dice al ALU la operación requerida en los datos y finalmente organiza los resultados devuelta a la memoria.
* Fetch: Esta unidad lleva las instrucciones desde la memoria principal y la pasa a la unidad execute. Esta unidad tiene un register que guarda el program counter (PC) que contiene a su vez la dirección de las instrucciones.
## Poder computacional del CPU

El poder computacional del CPU se ha incrementado exponencialmente. El número de transistores ha seguido aumentando por la ley de Moore desde 1970. La frecuencia de la CPU dejó de aumentar desde 2002. El rendimiento por núcleo continúa aumentando lentamente pero los aumentos se deben por innovaciones en el diseño y no por aumentar la frecuencia. La mayor contribución por chip desde 2002 viene de la tecnología multinúcleo.

## Administración de la memoria del CPU: Ocultar la latencia usando el Caché

Los datos e instrucciones no llegan al instante entre bloques. Estos avanzan clock-step por clock-step (Las pulsaciones del master clock) pasando por varios registros del hardware desde la fuente hasta su destino. Esta latencia sería contada como decenas de clock-cycles en una CPU y centenas de clock-cycles en una GPU. Afortunadamente, el desastre que puede provocar en rendimiento la latencia puede ser oculta usando una combinación de caching y pipelining. La idea es aprovechar el hecho de que los datos guardados de ubicaciones físicas de la memoria de manera secuencial son mayormente procesados secuencialmente en código. De este modo, cuando un dato es requerido por la unidad load en la memoria, el hardware de hecho envía este dato y el número de elementos adyacentes en sucesivos clock-ticks. Además, el dato requerido puede llegar con cierta latencia, después de eso,  los elementos sucesivos están disponibles en sucesivos clock-ticks.
En la práctica, las PCs usan una cantidad de memoria caché para almacenar los datos desde múltiples lugares y guardarlos en la memoria principal.
Las instrucciones del programa también se transmiten en una pipeline desde la memoria principal a la unidad fetch. Sin embargo, el pipeline romperá su flujo si existe una instrucción que genera una bifurcación (un if()), lo que hará que el hardware utilice trucos como la ejecución especulativa (speculative execution), para minimizar los efectos. En la ejecución especulativa, el PC ejecuta las instrucciones en uno o más caminos siguiendo la bifurcación antes de conocer el resultado de la misma para al final usar o descartar las instrucciones una vez el resultado de la bifurcación se conoce. No hace falta mencionar que para el hardware esto es un proceso complicado.

Existen tres niveles de la memoria caché, todas dentro del CPU. El primero es el level 3 (L3), con una cantidad de memoria caché de 8 MB y es compartida por 4 núcleos del CPU. Cada núcleo tiene caché L2 y L1, el cual L1 sirve para datos e instrucciones.
El hardware transfiere datos en caché en paquetes llamados cache lines con un peso normalmente de 64 o 128 bytes. Los datos en cache line vienen de un lugar de la memoria principal con una dirección que es un múltiplo del tamaño de la cache line.
## CPU: Parallel Instruction Set

Este punto es un poco de historia, se puede saltar pero es bastante interesante. Las CPUs de Intel ya tenían la capacidad de realizar instrucciones en paralelo pero lo hacían con vectores. Estas aparecieron por primera vez en 1999 con el Pentium III SSE instruction set (SSE significa Streaming SIMD Extensions el cual es un SIMD que significa Single Instruction Multiple Data). Estas instrucciones usaban 8 nuevos registros de 128-bit, cada uno podía guardar 4 floats de 4 bytes. Si se podían crear conjuntos de 4 floats que se pueden guardar en la memoria y capaz de ser almacenados dentro del límite de 128 bytes de memoria, estos podían ser tratados como vectores de 4 elementos y estos vectores podían ser guardados y cargados desde los registros SSE en un clock-cycle y asimismo los ALU podían realizar cálculos con vectores en un clock-cycle. De este modo, usando SSE se pudo mejorar enormemente los cálculos con floats en un factor de 4.
## Arquitectura del GPU

Las GPU fueron diseñadas para procesar gráficos de manera eficiente. En la era moderna, lo normal es tener una pantalla 1080p a 60 Hz. Cada píxel necesita estar ya calculado en el momento del juego y el punto de vista del jugador, algo así como 1.25 x 10^8 cálculos de pixel por segundo. Las tarjetas para gaming surgieron como hardware dedicado con múltiples procesadores para realizar los cálculos de los píxeles. Un detalle técnico importante es que el array del pixel representando la imágen es guardada en un array 2D en un digital frame buffer como datos, con 3 bytes por pixel que representan las intensidades del rojo, azul y verde de cada pixel (El RGB pues). El frame buffer (video ram) puede leer y escribir como cualquier memoria pero tienen un puerto adicional que permite hardware que se dedique al video para que se encargue de verificar los datos y enviárselos ya codificados para el monitor.
## Modelos de GPU de Nvidia

Nvidia tiene tres clases de GPU:
* Las GeForce GTX, GeForce RTX o las Titan. Estas son menos caras y se enfocan en el mercado de los videojuegos. Estos modelos tienen menos soporte para FP64 (float 64) en comparación a las versiones científicas y no tienen memoria ECC (Error code correction). Sin embargo, para calculos FP32 su rendimiento es igual o mejor que el de las tarjetas científicas. Los modelos Titan son los más poderosos.
* Tesla. Estos apuntan al mercado científico, tienen un buen soporte para FP64 y tienen memoria ECC. Las Tesla no tienen salida de video y no pueden ser usados para gaming. Estas tarjetas sirven mucho en server farms.
* Quadro: Son como los Tesla pero con capacidades gráficas y apuntan al mercado de alta gama.

Desde 2007 hasta 2020 (El momento en que se escribía este libro) Nvidia ha introducido 8 generaciones diferentes del GPU, cada una mejor que la anterior con mejores capacidades. Estas generaciones son llamadas con nombres de científicos famosos y entre cada generación existen modelos entre ellos que difieren en software.
## Arquitectura Pascal

1. La unidad básica es un simple compute-core (núcleo computacional simple), capaz de usar FP32 y operaciones con enteros. Estos cores no cuentan con PC individuales.
2. Grupos de 32 compute-cores forman lo que Nvidia llama “32-core processing blocks”. El autor menciona que los prefiere llamar “warp-engine” ya que funciona tal cual como los warps. El warp-engine añade recursos de computación adicionales compartidos entre los cores. Estas incluyen Special function units (SFUs o Unidades de Funciones Especiales) que son usadas para cálculos rápidos de funciones importantes como sin o exp, y variables double (FP64). En GPUs Pascal, los warp-engines tienen 8 SFU y hasta 16 o una unidad FP64.
3. A su vez, los warp-engines son agrupados juntos para formar lo que Nvidia llama “symmetric multiprocessors" o SMs. De este modo, una SM tiene normalmente 128 compute-cores. Los threads en un CUDA Kernel program son agrupados dentro de un número de thread blocks fijos. Cada thread block corre en solo un SM, pero diferentes thread blocks pueden correr en diferentes SMs. Aquí la razón del porque threads en el mismo block pueden comunicarse entre sí pero entre diferentes no. 
Además, estos añaden texture units (unidades de texturas), y varios chips de memoria que se reparten equitativamente entre warp-engines, incluyendo un register file de 64K 32-bit words, memoria compartida de 96 KB y L1/texture cache de 24 KB o 48 KB
5. Por último, un grupo de SM forman lo que conocemos como GPU.

Lo que hace el hardware es primero agrupar los cores en “warps” de 32 cores para después ser procesados y convertirse en un “Warp Engine”, que añade recursos incluyendo IO, doubles y SFUs. Grupos de Warp Engine, normalmente de 2 o 4, forman una unidad SM y finalmente grupos de SM forman al GPU.
## Tipos de memoria del GPU

La memoria del GPU se organiza de manera jerárquica muy similar a los cores.

* **Main memory**:  Este es análogo a la memoria principal del CPU. El programa en sí y todos los datos se encuentran aquí. La CPU puede escribir y leer datos para y desde la memoria principal del GPU. La transferencia de datos pasan por el bus PCI, son relativamente lentos, por lo que los programas de CUDA tienen que evitar la mayor cantidad de transferencias posibles. Afortunadamente los datos se guardan en la memoria principal de la GPU mediante Kernel calls, lo que permite reusar estos datos con más Kernel calls sin la necesidad de volver a cargar estos datos. También es posible realizar estas transferencias de datos en paralelo con las Kernel executions. No olvidar que tanto datos en la texture y en la constant memory se están guardando en la memoria del GPU y pueden ser reescritos por el CPU aunque sean read-only para la GPU.
* **Constant Memory**: 64 KB de la memoria principal se reservan para datos constantes. Constant Memory tiene un caché dedicado que pasa sobre la L2 cache, para poner un ejemplo, si todos los threads de un warp leen la misma ubicación de memoria, está puede leerlo tan rápido como si tuviera los datos en un register. El uso de const y restrict correctamente ayuda muchísimo.
* **Texture Memory**: Sirve para guardar arreglos de hasta 3 dimensiones y están optimizadas para acceder a la dirección local de arreglos en 2D. Estas son read-only y tienen su propia caché dedicada. Se puede acceder a las texturas con las special lookup functions, tex1D, tex2D y tex3D. Estas funciones son capaces realizar muy rápido interpolación 1D, interpolación bilineal 2D e interpolación trilineal 3D.
* **Local Memory**: Estos son bloques de memoria privados para cada thread en ejecución. Son usados como memoria extra para variables locales en resultados temporales intermedios cuando los register disponibles para el thread son insuficientes. El compilador sabe cuándo usar este recurso. La Local Memory es cacheada mediante las cachés L1 y L2 como un dato cualquiera.
* **Register File**: Cada SM tiene 64K 32-bit registers que a su vez se reparten equitativamente entre los thread blocks ejecutándose en el SM. De hecho, hay un límite de 64 como número máximo de warps (equivalente a 2K threads) que se pueden ejecutar en un SM asignado. Esto quiere decir que si el compilador permite que un thread use más de 32 registers, el máximo número de thread blocks corriendo en el SM se reduciría, afectando demasiado el rendimiento.
* **Shared Memory**: Cada SM tiene entre 32KB y 64 KB de Shared Memory. Si un Kernel requiere de la Shared Memory, este puede ser declarado al momento de lanzar el kernel o en el momento de la compilación. Cada thread block que se está ejecutando en un SM tiene el mismo tamaño de memoria. De este modo, si tu Kernel requiere de más de la mitad de la Shared Memory, la occupancy del SM se reducirá a un simple thread block por SM.

La Shared Memory es importante porque es muy rápida y porque provee la mejor manera para los threads sin thread block de comunicarse el uno con el otro.
Las GPU actuales usan el caché del L1 y L2 juntos llenándolos para esconder la latencia del acceso a la memoria principal.
## Warps y Waves

Diseñar buenos Kernels para resolver ciertos problemas requieren de gran habilidad y experiencia y eso es muy importante. Primero, necesitas decidir cuántos threads ocupas. Escoger la cantidad correcta de Nthreads es una de las decisiones más importantes al momento de usar CUDA Kernels. En el ejemplo ejecutando el programa gpusum se estableció Nthreads como la cantidad de pasos requeridos para el problema en cuestión. Para procesar una imagen en 2D teniendo x * y pixeles, una buena elección sería establecer Nthreads = x * y. El punto clave es establecer Nthreads lo más que se pueda.
Si esperas que estableciendo Nthreads = Ncores, el número de cores en tu GPU, será suficiente para mantener tu GPU totalmente ocupada. No puedes estar más lejos de la realidad; una de las características de las GPU de Nvidia es que el hardware puede ocultar la latencia al acceso de memoria o otros pipelines del hardware cambiando dinámicamente entre threads y el uso de los datos para ciertos threads tan pronto como se pueda.
Por ejemplo, una RTX 2070 tiene 36 SM (Nsm = 36) y cada SM tiene el hardware para procesar 2 warps de 32 threads (Nwarp = 2). De este modo, para esta GPU los Ncores = Nsm x Nwarp x 32 = 2304. Lo que no es tan obvio es que durante el proceso del Kernel cada SM tiene una gran cantidad de resident threads, Nres, en este caso, Nres = 1024 equivalente a 32 warps. En cualquier instante 2 de los 32 warps estarán activos mientras que el resto se quedarán en suspenso. Así es como funciona ocultar la latencia. Cuando lanzamos un Kernel de digamos 10^9 threads, estos threads correrán en waves de Nwave = Nres x Nsm threads; de hecho son 27127 waves en la GPU 2070 con Nwave = 36864, dejando la última wave incompleta. La idea es que el mínimo número de threads de cualquier Kernel debería ser Nwave, y si más threads son posibles, tiene que ser un múltiplo de Nwave.
## Blocks y Grids
En CUDA el concepto de Thread block es muy importante; es un grupo de threads que están juntos por lotes y corren en el mismo SM. El tamaño de los thread block debería ser un múltiplo del tamaño de warps (actualmente 32 para todas las GPU de Nvidia) hasta un máximo de 1024 por el hardware. En Kernel, threads en el mismo thread block pueden comunicarse entre sí usando la Shared o la Global device memory y puede sincronizarse con otros si es necesario. Threads en diferentes thread blocks no pueden comunicarse durante la ejecución de un Kernel y el sistema no puede sincronizar threads de distintos thread blocks.
Cuando ejecutamos un CUDA Kernel, especificamos la configuración de lanzamiento con 2 valores, el tamaño del thread block y el número de thread blocks. La documentación de CUDA se refiere a esto como ejecutar un grid de thread blocks y el grid-size es solo el número de thread blocks.
## Occupancy
Nvidia define la occupancy como el radio del número de threads que son residentes en las unidades SM en comparación con el máximo valor de Nres. La Occupancy se representa en porcentaje. Occupancy llena de 100% es lo mismo que decir que todas las waves están corriendo en las SMs del GPU.

# Pensando y escribiendo código en paralelo
## La taxonomía de Flynn
Las computadoras científicas reconocen la arquitectura en serie y paralelo de la computadora porque se describen con 4 letras acrónimos que se encuentran en la taxonomía de Flynn
* **SISD**: Single Instruction Single Data
* **SIMD**: Single Instruction Multiple Data
* **MIMD**: Multiple Instruction Multiple Data
* **MISD**: Multiple Instructions Single Data
* **SIMT**: Single Instruction Multiple Threads

Lo importante aquí es que MIMD sirve más para realizar acciones en paralelo en CPU, MISD es lo más raro que uno puede toparse y que lo usan los satélites por poner un ejemplo, mientras que Nvidia fue quien introdujo el SIMT como una variación del SIMD.
Las diferencias entre ellos es que SIMD una pequeña cantidad de threads usan vectores en hardware relativamente para procesar datos, mientras que, en el SIMT una gran cantidad de threads se encargan de procesar datos individuales de objetos. Si todos los threads se encargan de una instrucción en común entonces se estaría replicando el comportamiento del SIMD, pero la arquitectura SIMT permite que los threads realicen operaciones divergentes que, puede provocar una caída en rendimiento, también permite tener un código más versátil.

En el ejemplo de la suma en sin, como los loops presentados son totalmente independientes ya que no afecta el orden en que sumamos el resultado final, lo hace perfecto para ejecutarlo en paralelo. La variable sin_host tiene que ser global para que todos los threads participando en la ejecución encuentren la variable y obtener la suma correcta.
Pero hacerlo global trae un problema. Si dos threads quieren actualizar la variable simultáneamente el resultado sería indefinido. Con CUDA, un thread logrará realizar su operación mientras que el otro será ignorado, lo que hará que el resultado final sea incorrecto. Para resolver esto se utiliza algo llamado Atomic operations. Estas operaciones se encargan de llamar funciones específicas en la plataforma. Funciona pero puede retrasar un poco la tarea por lo que usaremos otra alternativa, que sería simplemente guardar el resultado retornado por la función sinsum en elementos separados de un array largo. Se sumarán todos los elementos del array en un step aparte una vez que todos sean calculados.

Definiciones de Keywords
* __device__: Le dice al compilador que compile una versión de la función que pueda ejecutar la GPU y que pueda ser llamado por los Kernels y otras funciones corriendo en la GPU
* __host__: Le dice al compilador que compile una versión de la función que se ejecuta únicamente en CPU
* inline: Le dice al compilador que genere una función ya incrustada en las calls del código, lo que remueve la sobrecarga de llamar a una función al costo de aumentar el tamaño del .exe final. En CUDA, inline viene por default para las funciones __device__.
* __global__: Le dice al compilador que genere una función que pueda ser llamada por la CPU pero ejecutada en GPU

La función sinsum en __device__ es solo una función en GPU y no puede ser llamada directamente por el host. Se requiere escribir una función CUDA Kernel que funcione en la GPU y pueda ser llamado desde el host. Los CUDA Kernels son declarados usando __global__ en lugar de __device__. En el mundo de CUDA la gente suele decir “launching” kernels en lugar de “calling” por lo que de ahora en adelante le diré así.
Sin embargo, las funciones en global tienen ciertas restricciones, como que a pesar de ser launcheados en host, no puede acceder a los datos del host. Todos los Kernel tienen que ser declarados como void y sus argumentos se restringen a valores escalares o apuntadores previamente alojados en la memoria del device. No se permiten crear referencias de los Kernels. Además, cualquier clase o estructura en C++ que usa un Kernel tiene que tener una versión __device__ de todas sus funciones.

Los valores de las variables blockDim.x, blockIdx.x y threadIdx.x dependerán de los parámetros declarados al momento de lanzar el kernel.

* blockDim.x lo establecerá la variable threads
* blockIdx.x establecerá el rango del thread block donde el thread actual se encuentra y estará en el rango entre [0, blocks - 1]
* threadIdx.x establecerá el rango del thread actual dentro de su thread block y estará en el rango entre [0, threads - 1]
* step = blockDim.x * blockIdx.x + threadIdx.x está en el rango [0, threads x blocks - 1].

El punto clave es que el sistema corra threads x blocks instancias del kernel en el GPU cubriendo todas las posibles combinaciones de los valores threadIdx y blockIdx. De este modo, cuando miramos a un kernel debemos imaginar que estamos mirando el contenido de un loop que está siendo ejecutado por todos los valores posibles que nos permiten las variables establecidas. Cuando miramos un Kernel, debemos imaginar que ese código está corriendo simultáneamente por todos los threads.

Para poder almacenar los datos en device, creamos el array dsums usando la clase thrust::device_vector<float> del tamaño de la cantidad de steps que tengamos. Por default el array inicializará los valores en 0 en device. No podemos pasar dsums directamente al kernel ya que no fué diseñado para eso, pero podemos pasar un apuntador que apunte a la memoria del array.

Launchear un kernel es básicamente llamar a una función con un <<<blocks, threads>>> en medio entre el nombre de la función y sus argumentos. Las variables int que aparecen aquí especifican la cantidad de threads por thread block y el número de thread blocks. Los threads tienen que ser múltiplos de 32 y su valor máximo es de 1024 para todas las GPU actuales.
## Sintaxis de una Kernel call
La forma de llamar a un CUDA Kernel usa hasta 4 argumentos en los brackets. Los 4 argumentos dentro de los brackets son:
* El primero: Define la dimensión del grid de thread blocks que usará el kernel.
* El segundo: Define el número de threads que tendrá un thread block.
* El tercero: Este es totalmente opcional, es de tipo int y define el número de bytes de Shared Memory alojada dinámicamente que usará cada thread block del kernel. No se reservará nada de Shared Memory si el argumento está vacío.
* El cuarto: Este es totalmente opcional, es de tipo cudaStream_t. Especificando al CUDA stream en dónde correr el kernel.
## Ocultar latencia y occupancy
Cuando un nuevo kernel comienza a ejecutarse, un warp de 32 threads comenzará su ejecución en cada uno de los warp-engine en la GPU. Estos warps se convierten en active-warps y permanecerán residents hasta que todos los warps en sus thread blocks estén completos. Un paso extra será cargar un dato desde la memoria global, pero la carga de este dato desde la memoria global tiene una latencia de cientos clock-cycles. Por lo tanto, los threads activos deben esperar a que lleguen sus datos antes de poder continuar; mientras esperan, se denominan stalled threads (threads detenidos). De hecho, el hardware es bastante sofisticado y los threads no se detendrán hasta que se alcance una instrucción que realmente use los datos pendientes. Esto significa que un programador puede ocultar parte de la latencia realizando trabajo independiente de los datos entre la solicitud de esos datos y su uso. Si bien es útil, puede haber un alcance limitado para esto en la práctica, y escribir instrucciones en un orden poco natural hace que el código sea más difícil de depurar y mantener, probablemente sea mejor confiar en el compilador para hacer este trabajo por ti.
Una característica clave del diseño de la GPU es que cada warp-engine puede procesar varios warps activos de manera intercalada; si uno de sus warps activos se detiene, un warp-engine cambiará a otro warp activo capaz de ejecutarse sin pérdida de ciclos. La conmutación eficiente entre warps es posible porque cada thread en un warp activo mantiene su propio estado y conjunto de registers. Esto contrasta drásticamente con la arquitectura de CPU estándar, donde un solo conjunto de registers es compartido por todos los threads activos, lo que hace que el cambio de tareas sea una operación costosa. Por lo tanto, un warp-engine puede continuar ejecutando instrucciones hasta que todos sus warps activos se detengan esperando el acceso a la memoria. Tal detención múltiple puede ocurrir una vez al comienzo de un kernel, pero, siempre que cada thread realice una "suficiente" cantidad de cómputo entre cada acceso a la memoria global, se evitan más detenciones. Ten en cuenta que una cantidad suficiente de cómputo es el número de instrucciones que podrían ejecutarse en el tiempo que lleva un “típico” acceso a la memoria. Esto puede ser varios cientos de instrucciones en una GPU moderna.


| Tamaño del thread block | Block por SM | Blocks por grid si GPU tiene 20 SMs | Registers por thread | Bytes de shared memory por thread block |
|-------------------------|--------------|-------------------------------------|----------------------|-----------------------------------------|
| 32                      | 64           | 1280                                | 32                   | 1 KB                                    |
| 64                      | 32           | 640                                 | 32                   | 2 KB                                    |
| 128                     | 16           | 320                                 | 32                   | 4KB                                     |
| 256                     | 8            | 160                                 | 32                   | 8 KB                                    |
| 512                     |  4           | 80                                  | 32                   | 16 KB                                   |
| 1024                    | 2            | 40                                  | 32                   | 32 KB                                   |



En la arquitectura Pascal, cada SM tiene 4 warp-engines, cada uno capaz de ejecutar hasta 16 warps activos, o lo que es equivalente, 64 warps o 2048 threads en el SM. Por lo tanto, si la latencia de la memoria global es de 400 ciclos, cada thread solo necesitaría realizar 25 ciclos de cómputo entre accesos a la memoria para ocultar completamente esta latencia. Esta es una situación ideal, ya que la configuración de inicio del kernel puede restringir el número máximo de warps activos a menos de 16. La occupancy de un kernel se define como:

occupancy = Número de warps activos / máximo número de warps.

Los factores que pueden limitar la occupancy son el tamaño de los thread blocks, el número de thread blocks, el número de registers utilizados por cada thread y la cantidad de shared memory utilizada por un thread block. El número de thread blocks debe ser un múltiplo entero del número de SM en la GPU, suficiente para proporcionar 2048 threads por block. También existen límites de 64K registers de 32 bits y 64 KB de shared memory por SM. Ten en cuenta que el número de registers asignados a cada thread es determinado por el compilador y depende de la complejidad del código. Para lograr una occupancy completa, el hardware tiene suficientes registers para 32 registers por thread.
La occupancy completa es más importante para kernels con limitaciones de memoria que para kernels con limitaciones computacionales, pero siempre es una buena idea mantener el código de tu kernel compacto y sencillo, ya que esto permitirá que el compilador asigne registers de manera más efectiva. También es una buena idea dividir cálculos largos en etapas y utilizar kernels separados para cada etapa. Recuerda que el contenido de la memoria global de la GPU se conserva entre lanzamientos de kernels.
## Patrones paralelos
Programar en paralelo para la GPU requiere de un vasto número de threads así que requiere que replanteemos al momento de escribir código.

### Evita declarar if
Los if son un problema enorme al momento de programar en CUDA. De manera resumida, suponiendo que tenemos un warp de 32 threads y declaramos un if donde solo un thread no cumple condicion1 mientras que los otros 31 si, nos encontramos frente a algo llamado branch-divergence. El sistema maneja esto mediante la serialización de las llamadas a las dos funciones, es decir, el subconjunto de threads en el warp que cumplen condicion1 ejecuta la llamada a function1 mientras que los threads no cumplen condicion1 permanecen inactivos. Luego, cuando function1 ha llegado a su return para todos los threads activos, la cláusula else que llama a function2 es ejecutada por los threads previamente inactivos mientras que los threads previamente activos ahora están inactivos
Parallel reduce
Considera el problema de sumar N números de floats almacenados en la memoria global de la GPU. El primer punto a reconocer es que cada elemento de datos solo requiere una simple suma; por lo tanto, estaremos limitados por la velocidad de acceso a la memoria y no por el rendimiento aritmético. Queremos utilizar tantos threads como sea posible para ocultar eficientemente la latencia de la memoria.

## Shared Memory
La shared memory es una memoria de acceso rápido de, normalmente, 64 KB disponible en cada SM. Los kernels pueden optar por usar la memoria compartida al declarar una o más variables de array o escalares con la decoración __shared__.
Cada thread block que se ejecuta en un SM recibe una asignación separada del tamaño requerido desde el pool de memoria compartida. Si un kernel tiene thread blocks que requieren más de 32 KB, entonces solo un bloque de threads puede ejecutarse en el SM a la vez, lo que reduce significativamente la occupancy. Por lo tanto, el uso de la shared memory es uno de los factores a considerar al optimizar la occupancy.
Como su nombre indica, la shared memory es compartida por todos los threads en un thread block, es decir, cualquier parte de ella puede ser leída o escrita por cualquier thread en el thread block. Ten en cuenta que, si bien esto es extremadamente útil en muchas situaciones, el intercambio es local: la shared memory no se comparte entre thread blocks. El contenido de la shared memory perteneciente a un thread block dado se pierde cuando ese thread block termina.
La asignación de shared memory puede ser estática o dinámica. Para la asignación estática de shared memory, los tamaños requeridos se declaran en el código del kernel utilizando valores conocidos en tiempo de compilación, por ejemplo, utilizando:

*__shared__ float data[256]*

Este es posiblemente el método más simple, pero carece de flexibilidad. Los tamaños de los arrays de shared memory suelen depender del número de threads en el thread block, por lo tanto, si están fijos en tiempo de compilación, también lo está el tamaño del thread block.
La asignación dinámica de shared memory es una alternativa en la que el kernel no especifica el tamaño de un array, sino que declara un puntero compartido asignado externamente, por ejemplo:

_extern __shared__ float *data_

# Conclusión

Sin lugar a dudas me ha aclarado demasiadas cosas y la relevancia de la programación paralela, así como lo importante que es conocer el hardware con el que trabajamos. He logrado entender cómo se compone una GPU, las capacidades de cada uno de sus componentes y con este conocimiento podría crear programas más optimizados en un futuro, ahora toca investigar cómo implementar CUDA en cualquier lugar como por ejemplo Unity o Unreal Engine.
La manera en que el autor muestra el tema es impresionante. Sus explicaciones y ejemplos ayudan bastante a la comprensión del mismo, normalmente suelo quedarme con sueño con estas cosas pero en este caso llamó demasiado mi atención que simplemente no me pude despegar de este libro en el momento en que empecé a escribir este resúmen.

# Referencias
R. Ansorge, Programming in parallel with CUDA: A practical guide. United Kingdom: TJ Books, 2022.
